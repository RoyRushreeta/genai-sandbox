{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c931182a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553c4195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:\n",
      "['\\nNatural Language Processing (NLP) is a fascinating field of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'The ultimate objective of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.', 'What a great technology!']\n",
      "\n",
      "Word Tokenization:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'The', 'ultimate', 'objective', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'valuable', 'way', '.', 'What', 'a', 'great', 'technology', '!']\n",
      "\n",
      "POS Tagging:\n",
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('through', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.'), ('The', 'DT'), ('ultimate', 'JJ'), ('objective', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('is', 'VBZ'), ('to', 'TO'), ('enable', 'JJ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), (',', ','), ('interpret', 'VB'), (',', ','), ('and', 'CC'), ('generate', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('in', 'IN'), ('a', 'DT'), ('valuable', 'JJ'), ('way', 'NN'), ('.', '.'), ('What', 'WP'), ('a', 'DT'), ('great', 'JJ'), ('technology', 'NN'), ('!', '.')]\n",
      "\n",
      "Tree Tokenization (Named Entity Chunking):\n",
      "(S\n",
      "  Natural/JJ\n",
      "  Language/NNP\n",
      "  Processing/NNP\n",
      "  (/(\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  fascinating/JJ\n",
      "  field/NN\n",
      "  of/IN\n",
      "  artificial/JJ\n",
      "  intelligence/NN\n",
      "  that/WDT\n",
      "  focuses/VBZ\n",
      "  on/IN\n",
      "  the/DT\n",
      "  interaction/NN\n",
      "  between/IN\n",
      "  computers/NNS\n",
      "  and/CC\n",
      "  humans/NNS\n",
      "  through/IN\n",
      "  natural/JJ\n",
      "  language/NN\n",
      "  ./.\n",
      "  The/DT\n",
      "  ultimate/JJ\n",
      "  objective/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  is/VBZ\n",
      "  to/TO\n",
      "  enable/JJ\n",
      "  computers/NNS\n",
      "  to/TO\n",
      "  understand/VB\n",
      "  ,/,\n",
      "  interpret/VB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  generate/VB\n",
      "  human/JJ\n",
      "  language/NN\n",
      "  in/IN\n",
      "  a/DT\n",
      "  valuable/JJ\n",
      "  way/NN\n",
      "  ./.\n",
      "  What/WP\n",
      "  a/DT\n",
      "  great/JJ\n",
      "  technology/NN\n",
      "  !/.)\n",
      "\n",
      "Tree Tokenization (Named Entity Chunking):\n",
      "(S\n",
      "  Natural/JJ\n",
      "  Language/NNP\n",
      "  Processing/NNP\n",
      "  (/(\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  fascinating/JJ\n",
      "  field/NN\n",
      "  of/IN\n",
      "  artificial/JJ\n",
      "  intelligence/NN\n",
      "  that/WDT\n",
      "  focuses/VBZ\n",
      "  on/IN\n",
      "  the/DT\n",
      "  interaction/NN\n",
      "  between/IN\n",
      "  computers/NNS\n",
      "  and/CC\n",
      "  humans/NNS\n",
      "  through/IN\n",
      "  natural/JJ\n",
      "  language/NN\n",
      "  ./.\n",
      "  The/DT\n",
      "  ultimate/JJ\n",
      "  objective/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  is/VBZ\n",
      "  to/TO\n",
      "  enable/JJ\n",
      "  computers/NNS\n",
      "  to/TO\n",
      "  understand/VB\n",
      "  ,/,\n",
      "  interpret/VB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  generate/VB\n",
      "  human/JJ\n",
      "  language/NN\n",
      "  in/IN\n",
      "  a/DT\n",
      "  valuable/JJ\n",
      "  way/NN\n",
      "  ./.\n",
      "  What/WP\n",
      "  a/DT\n",
      "  great/JJ\n",
      "  technology/NN\n",
      "  !/.)\n"
     ]
    }
   ],
   "source": [
    "# From a sample corpus, use NLTK library to do sentence and word tokenization\n",
    "sample_corpus = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human language in a valuable way. What a great technology!\n",
    "\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(sample_corpus)\n",
    "print(\"Sentence Tokenization:\")\n",
    "print(sentences)\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(sample_corpus)\n",
    "print(\"\\nWord Tokenization:\")\n",
    "print(words)\n",
    "\n",
    "# Part-of-speech tagging\n",
    "pos_tags = pos_tag(words)\n",
    "print(\"\\nPOS Tagging:\")\n",
    "print(pos_tags)\n",
    "\n",
    "# Tree (Chunking / Syntax Tree)\n",
    "tree = ne_chunk(pos_tags)\n",
    "print(\"\\nTree Tokenization (Named Entity Chunking):\")\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f87c2",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d232d17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['run', 'run', 'ran', 'easili', 'fairli', 'happi']\n",
      "Lancaster Stemmer: ['run', 'run', 'ran', 'easy', 'fair', 'happy']\n",
      "Snowball Stemmer: ['run', 'run', 'ran', 'easili', 'fair', 'happi']\n",
      "RegexpStemmer Output: ['runn', 'run', 'ran', 'easi', 'fair', 'happi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, RegexpStemmer\n",
    "\n",
    "# Sample words\n",
    "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\", \"happiness\"]\n",
    "\n",
    "# Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "porter_stems = [porter.stem(word) for word in words]\n",
    "print(\"Porter Stemmer:\", porter_stems)\n",
    "\n",
    "# Lancaster Stemmer\n",
    "lancaster = LancasterStemmer()\n",
    "lancaster_stems = [lancaster.stem(word) for word in words]\n",
    "print(\"Lancaster Stemmer:\", lancaster_stems)\n",
    "\n",
    "# Snowball Stemmer (English)\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "snowball_stems = [snowball.stem(word) for word in words]\n",
    "print(\"Snowball Stemmer:\", snowball_stems)\n",
    "\n",
    "# Regexp Stemmer\n",
    "stemmer = RegexpStemmer('(ing|ed|ly|ness|s)$')\n",
    "regex_stems = [stemmer.stem(word) for word in words]\n",
    "print(\"RegexpStemmer Output:\", regex_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2efcc2f",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5981672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words:  ['running', 'better', 'cats', 'geese', 'was', 'studies']\n",
      "POS Tags:  [('running', 'VBG'), ('better', 'JJR'), ('cats', 'NNS'), ('geese', 'VBP'), ('was', 'VBD'), ('studies', 'NNS')]\n",
      "Lemmatized Words:  ['run', 'good', 'cat', 'geese', 'be', 'study']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample words\n",
    "words = [\"running\", \"better\", \"cats\", \"geese\", \"was\", \"studies\"]\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map POS tags to WordNet POS\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "\n",
    "print(\"Original Words: \", words)\n",
    "print(\"POS Tags: \", pos_tags)\n",
    "print(\"Lemmatized Words: \", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267fd59e",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7386349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['This', 'is', 'a', 'simple', 'example', 'showing', 'how', 'to', 'remove', 'stopwords', 'from', 'a', 'sentence', 'using', 'NLTK', '.']\n",
      "After Stopword Removal: ['simple', 'example', 'showing', 'remove', 'stopwords', 'sentence', 'using', 'NLTK', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a simple example showing how to remove stopwords from a sentence using NLTK.\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Words:\", words)\n",
    "print(\"After Stopword Removal:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9052c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
